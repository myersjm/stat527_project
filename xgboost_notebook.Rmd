---
title: "XGBoost - Walmart"
output: html_notebook
---

We will explore the XGBoost method in this notebook.

### Load Packages and Data
```{r}
library(tidyverse)
library(xgboost)

# Load data:
df <- read_csv("./data/walmart_features.csv")
head(df)
```

### Data Preprocessing

We should first change the `store` and `holiday_flag` columns to factors, and then sort by date. We can also drop the variable `is_holiday_num`, since it contains the exact same information as `holiday_flag`.

```{r}
df <- df %>%
  mutate(
    store = as.factor(store), 
    holiday_flag = factor(holiday_flag, levels = unique(holiday_flag))
  ) %>% 
  arrange(date, store) %>% select(-is_holiday_num)

head(df)
```

Let's create a data set containing several different lags and moving averages for weekly sales.


### Cross-Validation and Train/Test Split

We will perform 5-fold time series cross-validation to select the best hyperparameters. In each training set, we standardize any columns related to the sales, such as lags and moving averages, by store. This is to prevent stores with high weekly sales from dominating the variable selection process.

We then scale these columns in each testing set using the corresponding mean and standard deviation of the training set.

```{r}
# function for standardizing training columns
standardize_sales_columns <- function(df, col_vec) {
  # standardize the columns, grouping by store first
  df_new <- df %>% group_by(store) %>% 
    mutate(across(any_of(col_vec), function(x) scale(x))) %>%
    ungroup()
  
  # store the mean and sd of the original columns (grouped by store)
  df_mean_sd <- df %>% group_by(store) %>%
    summarise(across(any_of(col_vec),
                     list(mean = mean, sd = sd),
                     na.rm = TRUE))
  
  list(df_new = df_new, df_mean_sd = df_mean_sd)
}

# function for standardizing test columns, given training column stats
scale_test_sales_columns <- function(df, col_vec, df_mean_sd) {
  df_new <- df %>% left_join(df_mean_sd, by="store")
  
  for(cname in col_vec) {
    cname_mean <- paste(cname, "mean", sep="_")
    cname_sd <- paste(cname, "sd", sep="_")
    
    df_new[,cname] <- (df_new[,cname] - df_new[,cname_mean])/ df_new[,cname_sd]
  }
  
  df_new %>% select(-any_of(colnames(df_mean_sd)[-1]))
}

# function to create data sets for time series CV
train_test_cv <- function(df, std_vec, nfolds=5) {
  dat <- list()
  
  dates <- sort(unique(df$date))
  nweeks <- length(dates)
  test_len <- ceiling(nweeks/(2*nfolds))
  idx <- nweeks - c(nfolds:1)*test_len
  
  # expanding window CV
  for (fold in seq(nfolds)) {
    train_idx <- idx[fold]
    test_idx <- idx[fold]+test_len
    
    train <- df %>% filter(date <= dates[train_idx])
    test <- df %>% filter(date > dates[train_idx], date <= dates[test_idx])
    
    # standardize the training columns
    std_cols <- standardize_sales_columns(train, std_vec)
    train_std <- std_cols$df_new
    train_stats <- std_cols$df_mean_sd
    
    # scale the test columns, excluding the response
    std_vec1 <- std_vec[std_vec != "weekly_sales"]
    test_std <- scale_test_sales_columns(test, std_vec1, train_stats)
    
    # return the mean and sd of Y to denormalize after prediction
    resp_stats <- train_stats %>% 
      select(store, weekly_sales_mean, weekly_sales_sd)
    
    
    trainX <- model.matrix(weekly_sales ~ ., train_std)
    testX <- model.matrix(weekly_sales ~ ., test_std)
    
    trainY <- train_std$weekly_sales 
    testYlab <- test_std %>% select(store, weekly_sales) # not standardized
    
    dat[[fold]] <- list(
      trainX=trainX, trainY=trainY,
      testX=testX, testYlab=testYlab,
      y_mean_sd=resp_stats
    )
  }
  
  dat
}

# variables to standardize
std_vec <- c("weekly_sales", "lag1", "lag2", "lag4", "lag8",
             "ma4", "ma8", "store_mean_to_prev", "store_sd_to_prev",
             "inter_holiday_lag1")

cv_data <- train_test_cv(df, std_vec)
```


### XGBoost
Let's fit an XGBoost on this data sets, using time series cross-validation with a grid search to find optimal hyperparameters. We use the WAPE (weighted absolute percentage error) metric to determine the best hyperparameters and compare models.

```{r}
set.seed(0)

library(xgboost)

# initialize parameter grid
param_grid <- expand.grid(
  max.depth = c(3, 6, 9),
  eta = c(0.05, 0.1, 0.25, 0.5),
  gamma = c(0, 0.25, 0.5),
  subsample = c(0.5, 1),
  colsample_bytree = c(0.5, 1)
)

# function to calculate WAPE
calculate_WAPE <- function(y, yhat) {
  sum(abs(y-yhat))/sum(abs(y))
}

nfolds <- 5 # 5 folds

n_rounds <- 200 # max rounds per training instance
n_earlystop <- 20 # early stopping rounds

WAPEs <- numeric(nrow(param_grid)) # store the WAPE

for(i in 1:nrow(param_grid)) {
  param_list <- lapply(param_grid[i,], function(x) x)
  
  WAPE_i <- numeric(nfolds)
  
  for(fold in seq(nfolds)) {
    fold_data <- cv_data[[fold]]
    
    trainX <- fold_data$trainX; trainY <- fold_data$trainY
    testX <- fold_data$testX; testYlab <- fold_data$testYlab
    y_mean_sd <- fold_data$y_mean_sd
    
    xgb1 <- xgboost(
      data = trainX, label = trainY, 
      params = param_list, nthread = 10, 
      nrounds = n_rounds, early_stopping_rounds = n_earlystop,
      objective = "reg:squarederror", verbose = 0
    )
    
    # create and denormalize the prediction
    pred <- predict(xgb1, testX)
    joined_yhat <- testYlab %>% left_join(y_mean_sd, by = "store") %>%
      mutate(pred_denorm = (pred * weekly_sales_sd) + weekly_sales_mean)
    
    yhat <- joined_yhat$pred_denorm
    
    WAPE_i[fold] <- calculate_WAPE(testYlab$weekly_sales, yhat)
  }
  
  # store the WAPE
  WAPEs[i] <- mean(WAPE_i)
}

min(WAPEs) # smallest WAPE
opt_params <- param_grid[which.min(WAPEs),]
opt_params # optimal parameter set
```

After obtaining the optimal hyperparameters from the grid search, we can train a final XGBoost model with them, using the final fold.

```{r}
opt_param_list <- lapply(opt_params, function(x) x)

xgb_opt <- xgboost(
  data = trainX, label = trainY, 
  params = opt_param_list, nthread = 10, 
  nrounds = n_rounds, early_stopping_rounds = n_earlystop,
  objective = "reg:squarederror", verbose = 0
)
```

### Variable Importance

```{r}
xgb.importance(model=xgb_opt)
```

The most important variable is `week`, followed by `lag1` and `lag4`. The stores were not imporant, which makes sense because we standardized the weekly sales by store before running the algorithm, so the effect of the store was not captured.

