---
title: "XGBoost - Walmart"
output: html_notebook
---

We will explore the XGBoost method in this notebook.

### Load Packages and Data
```{r}
library(tidyverse)
library(xgboost)

# Load data:
df <- read_csv("./data/walmart_features.csv")
head(df)
```

### Data Preprocessing

We should first change the `store` and `holiday_flag` columns to factors, and then sort by date. We can also drop the variable `is_holiday_num`, since it contains the exact same information as `holiday_flag`.

```{r}
df <- df %>%
  mutate(
    store = as.factor(store), 
    holiday_flag = factor(holiday_flag, levels = unique(holiday_flag))
  ) %>% 
  arrange(date, store) %>% select(-is_holiday_num)

head(df)
```


### Cross-Validation and Train/Validation Split

We will perform 5-fold time series cross-validation to select the best hyperparameters. In each training set, we standardize any columns related to the sales, such as lags and moving averages, by store. This is to prevent stores with high weekly sales from dominating the variable selection process.

We then scale these columns in each validation set using the corresponding mean and standard deviation of the training set.

```{r}
# function for standardizing training columns
standardize_sales_columns <- function(df, col_vec) {
  # standardize the columns, grouping by store first
  df_new <- df %>% group_by(store) %>% 
    mutate(across(any_of(col_vec), function(x) scale(x))) %>%
    ungroup()
  
  # store the mean and sd of the original columns (grouped by store)
  df_mean_sd <- df %>% group_by(store) %>%
    summarise(across(any_of(col_vec),
                     list(mean = mean, sd = sd),
                     na.rm = TRUE))
  
  list(df_new = df_new, df_mean_sd = df_mean_sd)
}

# function for standardizing test columns, given training column stats
scale_test_sales_columns <- function(df, col_vec, df_mean_sd) {
  df_new <- df %>% left_join(df_mean_sd, by="store")
  
  for(cname in col_vec) {
    cname_mean <- paste(cname, "mean", sep="_")
    cname_sd <- paste(cname, "sd", sep="_")
    
    df_new[,cname] <- (df_new[,cname] - df_new[,cname_mean])/ df_new[,cname_sd]
  }
  
  df_new %>% select(-any_of(colnames(df_mean_sd)[-1]))
}
```

With the creation of these two scaling functions, we can proceed with the train/validation split function for cross-validation. In fold $i$, data from the first $10\times(i+4)\%$ are used for training, and data from the following $10\%$ of weeks are used for validation, for $i=1,...,5.$

```{r, warning=FALSE}
# function to create data sets for time series CV
train_test_cv <- function(df, std_vec, nfolds=5) {
  dat <- list()
  
  dates <- sort(unique(df$date))
  nweeks <- length(dates)
  test_len <- ceiling(nweeks/(2*nfolds))
  idx <- nweeks - c(nfolds:1)*test_len
  
  # expanding window CV
  for (fold in seq(nfolds)) {
    train_idx <- idx[fold]
    test_idx <- idx[fold]+test_len
    
    train <- df %>% filter(date <= dates[train_idx])
    test <- df %>% filter(date > dates[train_idx], date <= dates[test_idx])
    
    # standardize the training columns
    std_cols <- standardize_sales_columns(train, std_vec)
    train_std <- std_cols$df_new
    train_stats <- std_cols$df_mean_sd
    
    # scale the test columns, excluding the response
    std_vec1 <- std_vec[std_vec != "weekly_sales"]
    test_std <- scale_test_sales_columns(test, std_vec1, train_stats)
    
    # return the mean and sd of Y to denormalize after prediction
    resp_stats <- train_stats %>% 
      select(store, weekly_sales_mean, weekly_sales_sd)
    
    
    trainX <- model.matrix(weekly_sales ~ ., train_std)
    testX <- model.matrix(weekly_sales ~ ., test_std)
    
    trainY <- train_std$weekly_sales 
    testYlab <- test_std %>% select(store, weekly_sales) # not standardized
    
    dat[[fold]] <- list(
      trainX=trainX, trainY=trainY,
      testX=testX, testYlab=testYlab,
      y_mean_sd=resp_stats
    )
  }
  
  dat
}

# variables to standardize
std_vec <- c("weekly_sales", "lag1", "lag2", "lag4", "lag8",
             "ma4", "ma8", "store_mean_to_prev", "store_sd_to_prev",
             "inter_holiday_lag1")

cv_data <- train_test_cv(df, std_vec)
```


### XGBoost
Let's fit an XGBoost on this data sets, using time series cross-validation with a grid search to find optimal hyperparameters. We use the WAPE (weighted absolute percentage error) metric to determine the best hyperparameters and compare models.

```{r, warning=FALSE}
set.seed(0)

library(xgboost)

# initialize parameter grid
param_grid <- expand.grid(
  max.depth = c(3, 6, 9),
  eta = c(0.05, 0.1, 0.25),
  gamma = c(0.1, 1, 5),
  subsample = c(0.5, 1),
  colsample_bytree = c(0.5, 1)
)

# function to calculate WAPE
calculate_WAPE <- function(y, yhat) {
  sum(abs(y-yhat))/sum(abs(y))
}

nfolds <- 5 # 5 folds

n_rounds <- 200 # max rounds per training instance

WAPEs <- matrix(0, nrow=nrow(param_grid), ncol=nfolds) # store the WAPE

for(i in 1:nrow(param_grid)) {
  param_list <- lapply(param_grid[i,], function(x) x)
  
  for(fold in seq(nfolds)) {
    fold_data <- cv_data[[fold]]
    
    trainX <- fold_data$trainX; trainY <- fold_data$trainY
    testX <- fold_data$testX; testYlab <- fold_data$testYlab
    y_mean_sd <- fold_data$y_mean_sd
    
    xgb1 <- xgboost(
      data = trainX, label = trainY, 
      params = param_list, nthread = 0, 
      nrounds = n_rounds,
      objective = "reg:squarederror", verbose = 0
    )
    
    # create and denormalize the prediction
    pred <- predict(xgb1, testX)
    joined_yhat <- testYlab %>% left_join(y_mean_sd, by = "store") %>%
      mutate(pred_denorm = (pred * weekly_sales_sd) + weekly_sales_mean)
    
    yhat <- joined_yhat$pred_denorm
    
    WAPEs[i,fold] <- calculate_WAPE(testYlab$weekly_sales, yhat)
  }
}

WAPE_means <- rowMeans(WAPEs)

min(WAPE_means) # smallest WAPE
opt_params <- param_grid[which.min(WAPE_means),]
opt_params # optimal parameter set
```

### Final Train/Test Split

After obtaining the optimal hyperparameters from the grid search, we can train a final XGBoost model with them, using data from the first 80% of the weeks to train, and the remaining 20% to test. The resulting WAPE from the test data will be compared against other models.

```{r}
# final train/test split
all_dates <- unique(df$date) # already sorted
cutoff_ix <- floor(0.8 * length(all_dates))
cutoff_date <- all_dates[cutoff_ix]

train_df <- df %>% filter(date <= cutoff_date)
test_df  <- df %>% filter(date >  cutoff_date)

# standardize the training columns
std_cols <- standardize_sales_columns(train_df, std_vec)
train_std <- std_cols$df_new
train_stats <- std_cols$df_mean_sd

# scale the test columns, excluding the response
std_vec1 <- std_vec[std_vec != "weekly_sales"]
test_std <- scale_test_sales_columns(test_df, std_vec1, train_stats)

resp_stats <- train_stats %>% 
  select(store, weekly_sales_mean, weekly_sales_sd)

trainX <- model.matrix(weekly_sales ~ ., train_std)
testX <- model.matrix(weekly_sales ~ ., test_std)

trainY <- train_std$weekly_sales 
testYlab <- test_std %>% select(store, weekly_sales)
```

We are ready to train and test the model.

```{r}
set.seed(5)

opt_param_list <- lapply(opt_params, function(x) x)

xgb_opt <- xgboost(
  data = trainX, label = trainY, 
  params = opt_param_list, nthread = 10, 
  nrounds = n_rounds, early_stopping_rounds = n_earlystop,
  objective = "reg:squarederror", verbose = 0
)

pred <- predict(xgb_opt, testX)
joined_yhat <- testYlab %>% left_join(y_mean_sd, by = "store") %>%
  mutate(pred_denorm = (pred * weekly_sales_sd) + weekly_sales_mean)

yhat <- joined_yhat$pred_denorm
  
calculate_WAPE(testYlab$weekly_sales, yhat)
```


### Variable Importance

```{r}
importance <- xgb.importance(model=xgb_opt)
importance

xgb.plot.importance(importance, top_n=10, cex=0.7)
```

The most important variable is `week`, followed by `lag4` and `lag1`. The stores were not imporant, which makes sense because we standardized the weekly sales by store before running the algorithm, so the effect of the store was not captured. Other unimportant variables were `unemployment` and `year`.



