# install.packages("tidyverse")
# install.packages("timetk")
library(tidyverse)
library(readr) #read_csv()
library(dplyr)
library(glmnet)
library(caret) # may not need anymore
library(timetk)
library(rsample)
# Load data:
df <- read_csv("./data/walmart/Walmart.csv")
head(df)
# Fixing datatypes:
# any(grepl("/", df$Date))
print(sapply(df, class))
df$Date <- gsub("/", "-", df$Date) # replace slashes with dashes, so all dates follow same format
df$Date <- as.Date(df$Date, format = "%d-%m-%Y") # NOTE european-like date: day/month/year
print(sapply(df, class))
# Sort values:
df <- df[order(df$Date, decreasing = FALSE), ] # gets an ordering of rows based on date and then df[ix] to select that order
head(df)
print(dim(df))
table(df$Date) # value_counts
df <- df %>%
group_by(Store) %>%
mutate(
sales_mean = mean(Weekly_Sales, na.rm = TRUE),
sales_std   = sd(Weekly_Sales, na.rm = TRUE),
weekly_sales_scaled = (Weekly_Sales - sales_mean) / sales_std
) %>%
ungroup()
head(df)
for (store_id in 1:length(unique(df$Store))) {
p <- df %>%
filter(Store == store_id) %>%
ggplot(aes(x = Date, y = weekly_sales_scaled)) +
geom_line(color = "steelblue") +
labs(title = paste("Weekly Sales Over Time - Store", store_id),
x = "Date",
y = "Weekly Sales (scaled per store)")
print(p)
}
# (Can't randomize the data first, because it is time-series)
# Train/Test Split:
split_ix <- floor(.8*nrow(df))
train_df <- df[1:split_ix,]
test_df <- df[(split_ix+1):nrow(df),]
print(dim(train_df))
print(dim(test_df))
cat("train:", format(min(train_df$Date), "%Y-%m-%d"), " to ", format(max(train_df$Date), "%Y-%m-%d"),"\n")
cat("test:", format(min(test_df$Date), "%Y-%m-%d"), " to ", format(max(test_df$Date), "%Y-%m-%d"),"\n")
print(colnames(df))
# Define design matrix pattern (to make it easier to form X's during cross val):
design_matrix_formula <- weekly_sales_scaled ~ . - Store - Date - Weekly_Sales - sales_mean - sales_std + 0 # 0 means no intercept, glmnet will add its own; here we drop columns
# model.matrix creates a design matrix from formula, auto-encodes categorical as dummy vars (if any--in this case, holiday is already a double.)
X_test <- model.matrix(design_matrix_formula, data=test_df)
y_test <- test_df$weekly_sales_scaled
print(dim(X_test))
print(X_test[1:10,]) # see example of columns left for prediction
# NOTE - ctrl shift c to mass uncomment/comment out
# Attempt 1 - shorter
# cv_folds <- time_series_cv(
#   data = train_df,
#   date_var = Date,
#   cumulative = FALSE, # no growing window (same size folds)
#   initial = "6 months", # length of train window
#   assess = "3 months", # length of validation window
#   skip = "3 months", # jump between folds
#   slice_limit = 5 # num folds max
# )
# Attempt 2-- longer, cumulative windowing
cv_folds <- time_series_cv(
data = train_df,
date_var = Date,
cumulative = TRUE, # growing window (same size folds)
initial = "1 year", # length of train window
assess = "3 months", # length of validation window
skip = "3 months", # jump between folds
slice_limit = 5 # num folds max
)
plot_time_series_cv_plan(cv_folds, .date_var = Date, .value = weekly_sales_scaled, .interactive=TRUE)
# === Define Lambda search grid ===
# Easy way to get lambda grid with good scaling--from the glmnet function itself.
# Note that we will not keep this model fit, this is solely for the purpose of getting
# a lambda grid for our manual CV.
unused_X_all <- model.matrix(design_matrix_formula, data=train_df)
unused_y_all <- train_df$weekly_sales_scaled
unused_model <- glmnet(unused_X_all, unused_y_all, alpha=1)
lambda_grid <- unused_model$lambda
lambda_grid
n_folds <- length(cv_folds$splits)
n_lambdas <- length(lambda_grid)
cv_mses <- numeric(n_lambdas) # keep track of MSE across different lambdas
lambda_ix <- 1
for (lambda in lambda_grid) {
fold_mses <- numeric(n_folds) # array for keeping track of mse across folds
for (fold_ix in 1:n_folds) {
# Get train, valid data for current cv fold:
fold <- cv_folds$splits[[fold_ix]]
train_data <- analysis(fold)
valid_data <- assessment(fold)
# Manipulate data into design matrices:
X_train <- model.matrix(design_matrix_formula, data=train_data)
y_train <- train_data$weekly_sales_scaled
X_val <- model.matrix(design_matrix_formula, data=valid_data)
y_val <- valid_data$weekly_sales_scaled
# Fit LASSO (non-cv version):
lasso_model <- glmnet(X_train, y_train, alpha=1, lambda=lambda)
# Get val error:
yhat_val <- predict(lasso_model, newx=X_val, s=lambda)
fold_mses[fold_ix] <- mean((y_val - yhat_val)^2)
}
cv_mses[lambda_ix] <- mean(fold_mses)
lambda_ix <- lambda_ix + 1
}
best_lambda <- lambda_grid[which.min(cv_mses)]
cat("best lambda: ", best_lambda, "with min mse ", min(cv_mses), "\n\n")
plot(x=lambda_grid,
y=cv_mses,
main = paste0(n_folds, "-Fold Cross-Validation MSE for different Lambdas"),
xlab = "Lambda",
ylab = "Mean CV MSE")
#best_ix <- which.min(cv_mses)
#points(best_lambda, cv_mses[best_ix], col = "red", pch = 19, cex = 1.5)
#text(best_lambda, cv_mses[best_ix],
#     labels = paste("Best Î» =", round(best_lambda, 4)),
#     pos = 4, col = "red")
X_train <- model.matrix(design_matrix_formula, data=train_df)
y_train <- train_df$weekly_sales_scaled
best_lasso_model <- glmnet(X_train, y_train, alpha=1, lambda=best_lambda)
# Get val error:
yhat_test <- predict(best_lasso_model, newx=X_test, s=best_lambda)
mse_test <- mean((y_test - yhat_test)^2)
cat("Test MSE: ", mse_test, "\n\n")
coef(best_lasso_model)
plot(best_lasso_model, xvar = "lambda")
fit_path <- glmnet(X_train, y_train, alpha = 1)  # doesnt take best lambda, fits a bunch of lambdas, no CV
plot(fit_path, xvar = "lambda", label = TRUE)
title("LASSO Coefficient Paths")
# 1) Extract coefficients for each tested lambda value, and convert datatype to R matrix:
Beta_lambda <- as.matrix(coef(fit_path, s=fit_path$lambda))
# 2) Even tho I didn't specify intercept, there is a intercept row of 0's.
#Beta_lambda <- Beta_lambda[-1,, drop=FALSE] # all rows except row 1, all columns; dont change dimensions
# print(Beta_lambda)
cat('Beta_lambda shape:', dim(Beta_lambda), '\n') # shape: (p x #lambdas
# 3) Get L0, L1 norms:
l1_norm = colSums(abs(Beta_lambda)) # sum up each betah_lambda col; colSums computes sum of each column
l0_norm = colSums(Beta_lambda != 0) # number of nonzero coeffs for each lambda
# 4) Want the plot's x axis to be increasing model complexity (incr L1 norm to the right)
incr_order_ixs = order(l1_norm) # returns indices to make it incr order
l1_norm_ordered = l1_norm[incr_order_ixs]
l0_norm_ordered = l0_norm[incr_order_ixs]
Beta_lambda_ordered = Beta_lambda[,incr_order_ixs,drop=FALSE]
# 5) Plot coeff vs. L1 norm:
# note: currently, Beta_lambda's columns are for each lambda, but matplot plots columns of y
#       as separate lines. We want a line for each coeff (p). Beta_lambda is (p x #lambdas).
#       so, we take the transpose of beta.
#       also, num rows should match for x (#lambdas x 1) and y (p x #lambdas).
cat('L1 norm shape:', length(l1_norm_ordered), '\n')
cat('Beta_lambda shape:', dim(Beta_lambda_ordered), '\n')
matplot(x=l1_norm,  y=t(Beta_lambda),
type="l",# plot type: line
lwd=2, # line width
lty=1, # line type (solid, dashed)
xlab=expression("L1 Norm (" * "||" * hat(beta[lambda]) * "||"[1] * ")"),
ylab="Coefficients",
main="Lasso Path")
abline(h = 0, col = "black", lwd = 1, lty = 2) # show x axis more easily
fit_path <- glmnet(X_train, y_train, alpha = 1)  # doesnt take best lambda, fits a bunch of lambdas, no CV
plot(fit_path, xvar = "lambda", label = TRUE)
title("LASSO Coefficient Paths")
print(fit_path$coefs)
fit_path <- glmnet(X_train, y_train, alpha = 1)  # doesnt take best lambda, fits a bunch of lambdas, no CV
plot(fit_path, xvar = "lambda", label = TRUE)
title("LASSO Coefficient Paths")
print(fit_path$coefficients)
fit_path <- glmnet(X_train, y_train, alpha = 1)  # doesnt take best lambda, fits a bunch of lambdas, no CV
plot(fit_path, xvar = "lambda", label = TRUE)
title("LASSO Coefficient Paths")
print(coef(fit_path))
fit_path <- glmnet(X_train, y_train, alpha = 1)  # doesnt take best lambda, fits a bunch of lambdas, no CV
plot(fit_path, xvar = "lambda", label = TRUE)
title("LASSO Coefficient Paths")
beta_mat <- as.matrix(fit_path$beta)
#rownames(beta_mat)
variable_map <- data.frame(
index = seq_len(nrow(beta_mat)),
variable = rownames(beta_mat)
)
print(variable_map)
