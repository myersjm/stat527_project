---
title: "Lasso - Walmart"
output: html_notebook
---

```{r}
# install.packages("tidyverse")
# install.packages("timetk")
library(tidyverse)
library(readr) #read_csv()
library(dplyr)
library(glmnet)
library(caret) # may not need anymore
library(timetk)
library(rsample)
```


### 1) Load Data
```{r}
# Load data:
df <- read_csv("./data/walmart/Walmart.csv")
head(df)
```

**Fix Datatypes, Sort Values**
```{r}
# Fixing datatypes:
# any(grepl("/", df$Date))
print(sapply(df, class))
df$Date <- gsub("/", "-", df$Date) # replace slashes with dashes, so all dates follow same format
df$Date <- as.Date(df$Date, format = "%d-%m-%Y") # NOTE european-like date: day/month/year
print(sapply(df, class))

# Sort values:
df <- df[order(df$Date, decreasing = FALSE), ] # gets an ordering of rows based on date and then df[ix] to select that order
head(df)
```
```{r}
print(dim(df))
table(df$Date) # value_counts
```

**There are 45 stores, so this makes sense, that there are 45 entries for each date. Also, each date represents one week.**


**Normalize weekly sales within each store, so that no one store dominates, and weekly sales are comparable:**
i.e. (sales_store - mean(sales_store)) / std(sales_store)
and can convert back later via (scaled_sales * store_std) + store_mean
```{r}
df <- df %>%
  group_by(Store) %>%
  mutate(
    sales_mean = mean(Weekly_Sales, na.rm = TRUE),
    sales_std   = sd(Weekly_Sales, na.rm = TRUE),
    weekly_sales_scaled = (Weekly_Sales - sales_mean) / sales_std
  ) %>%
  ungroup()
head(df)
```

```{r}
for (store_id in 1:length(unique(df$Store))) {
  p <- df %>%
    filter(Store == store_id) %>%
    ggplot(aes(x = Date, y = weekly_sales_scaled)) +
    geom_line(color = "steelblue") +
    labs(title = paste("Weekly Sales Over Time - Store", store_id),
         x = "Date",
         y = "Weekly Sales (scaled per store)")
  print(p)
  }


```

### 2) Train/Test split on sorted data:
```{r}
# (Can't randomize the data first, because it is time-series)
# Train/Test Split:
split_ix <- floor(.8*nrow(df))
train_df <- df[1:split_ix,]
test_df <- df[(split_ix+1):nrow(df),]

print(dim(train_df))
print(dim(test_df))
cat("train:", format(min(train_df$Date), "%Y-%m-%d"), " to ", format(max(train_df$Date), "%Y-%m-%d"),"\n")
cat("test:", format(min(test_df$Date), "%Y-%m-%d"), " to ", format(max(test_df$Date), "%Y-%m-%d"),"\n")
print(colnames(df))

# Define design matrix pattern (to make it easier to form X's during cross val):
design_matrix_formula <- weekly_sales_scaled ~ . - Store - Date - Weekly_Sales - sales_mean - sales_std + 0 # 0 means no intercept, glmnet will add its own; here we drop columns

# model.matrix creates a design matrix from formula, auto-encodes categorical as dummy vars (if any--in this case, holiday is already a double.)
X_test <- model.matrix(design_matrix_formula, data=test_df)
y_test <- test_df$weekly_sales_scaled

print(dim(X_test))
print(X_test[1:10,]) # see example of columns left for prediction
```

### 3) LASSO with Rolling Time-series cross validation
Because, it is important to keep the future in the test, or else it could cheat and 
accidentally learn the past and use it to predict the past, some inherent pattern.
i.e. could use info from the future to predict the past (leakage).

```{r}
cv_folds <- time_series_cv(
  data = train_df,
  date_var = Date,
  cumulative = FALSE, # no growing window (same size folds)
  initial = "6 months", # length of train window
  assess = "3 months", # length of validation window
  skip = "3 months", # jump between folds
  slice_limit = 5 # num folds max
)

plot_time_series_cv_plan(cv_folds, .date_var = Date, .value = weekly_sales_scaled, .interactive=TRUE)
```

```{r}
# === Define Lambda search grid ===
# Easy way to get lambda grid with good scaling--from the glmnet function itself.
# Note that we will not keep this model fit, this is solely for the purpose of getting
# a lambda grid for our manual CV.
unused_X_all <- model.matrix(design_matrix_formula, data=train_df)
unused_y_all <- train_df$weekly_sales_scaled
unused_model <- glmnet(unused_X_all, unused_y_all, alpha=1)
lambda_grid <- unused_model$lambda
lambda_grid
```


```{r}
n_folds <- length(cv_folds$splits)
n_lambdas <- length(lambda_grid)
cv_mses <- numeric(n_lambdas) # keep track of MSE across different lambdas
lambda_ix <- 1

for (lambda in lambda_grid) {
  fold_mses <- numeric(n_folds) # array for keeping track of mse across folds
  
  for (fold_ix in 1:n_folds) {
    # Get train, valid data for current cv fold:
    fold <- cv_folds$splits[[fold_ix]]
    train_data <- analysis(fold)
    valid_data <- assessment(fold)
    
    # Manipulate data into design matrices:
    X_train <- model.matrix(design_matrix_formula, data=train_data)
    y_train <- train_data$weekly_sales_scaled
    X_val <- model.matrix(design_matrix_formula, data=valid_data)
    y_val <- valid_data$weekly_sales_scaled
    
    # Fit LASSO (non-cv version):
    lasso_model <- glmnet(X_train, y_train, alpha=1, lambda=lambda)
    
    # Get val error:
    yhat_val <- predict(lasso_model, newx=X_val, s=lambda)
    fold_mses[fold_ix] <- mean((y_val - yhat_val)^2)
  }
  cv_mses[lambda_ix] <- mean(fold_mses)
  lambda_ix <- lambda_ix + 1
  
}

best_lambda <- lambda_grid[which.min(cv_mses)]
cat("best lambda: ", best_lambda, "with min mse ", min(cv_mses), "\n\n")
```

```{r}
plot(x=lambda_grid,
     y=cv_mses,
     main = paste0(n_folds, "-Fold Cross-Validation MSE for different Lambdas"),
     xlab = "Lambda",
     ylab = "Mean CV MSE")
#best_ix <- which.min(cv_mses)
#points(best_lambda, cv_mses[best_ix], col = "red", pch = 19, cex = 1.5)
#text(best_lambda, cv_mses[best_ix],
#     labels = paste("Best Î» =", round(best_lambda, 4)),
#     pos = 4, col = "red")
```
### 4) Fit model with best lambda on full data, get test error:
```{r}
X_train <- model.matrix(design_matrix_formula, data=train_df)
y_train <- train_df$weekly_sales_scaled
best_lasso_model <- glmnet(X_train, y_train, alpha=1, lambda=best_lambda)

 # Get val error:
yhat_test <- predict(best_lasso_model, newx=X_test, s=best_lambda)
mse_test <- mean((y_test - yhat_test)^2)
cat("Test MSE: ", mse_test, "\n\n")
coef(best_lasso_model)
```



### 4) LASSO - baseline, NO lagged variables.
```{r}

```

